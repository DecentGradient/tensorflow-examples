{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 1.2 seq2seq example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there seem to be a dearth of up-to-date tensorflow examples on how to use the seq2seq module in contrib, I've decided to post this code online. It is based primarily on this tutorial: [Udacity's sequence to sequence implementation](https://github.com/udacity/deep-learning/blob/master/seq2seq/sequence_to_sequence_implementation.ipynb)\n",
    "\n",
    "This example takes a list of numbers and sorts it. There are multiple updates from the Udacity example, such as scheduled sampling, beam search, and error rate calculation. You will best understand what is going on in this example code if you already have a good background in TensorFlow and seq2seq networks.\n",
    "\n",
    "Unfortunately, Jupyter doesn't work well with classes, so I will have to put most of the code in a single cell. The comments should describe what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.contrib.seq2seq.python.ops import beam_search_decoder as bsd\n",
    "import numpy as np\n",
    "\n",
    "class seq2seq_example:\n",
    "\n",
    "    # Constants\n",
    "    tokens         = {\"PAD\": 0, \"EOS\": 1, \"GO\": 2, \"UNK\": 3}\n",
    "    minLength      = 5\n",
    "    maxLength      = 10\n",
    "    samples        = 10000\n",
    "    vocab_size     = 50\n",
    "    embedding_size = 15\n",
    "    dropout        = 0.3\n",
    "    layers         = 2\n",
    "    layer_size     = 100\n",
    "    batch_size     = 50\n",
    "    beam_width     = 4\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Random integers up to the vocab_size (not including reserved integers)\n",
    "        self.data = np.random.randint(\n",
    "            low  = len(self.tokens),\n",
    "            high = self.vocab_size,\n",
    "            size = (self.samples, self.maxLength))\n",
    "        \n",
    "        # Assign a random length to each sequence from minLength to maxLength\n",
    "        self.dataLens = np.random.randint(\n",
    "            low  = self.minLength,\n",
    "            high = self.maxLength,\n",
    "            size = self.samples)\n",
    "        \n",
    "        # Create labels by sorting the original data\n",
    "        self.dataLabels = np.ones_like(self.data) * self.tokens['PAD']\n",
    "        for i in range(len(self.data)):\n",
    "            self.data[i, self.dataLens[i]:] = self.tokens['PAD']\n",
    "            self.dataLabels[i, :self.dataLens[i]] = np.sort(self.data[i, :self.dataLens[i]])\n",
    "       \n",
    "        # Make placeholders and stuff\n",
    "        self.make_inputs()\n",
    "\n",
    "        # Build the compute graph\n",
    "        self.build_graph()\n",
    "\n",
    "    # Create the inputs to the graph (placeholders and stuff)\n",
    "    def make_inputs(self):\n",
    "        self.input     = tf.placeholder(tf.int32, (self.batch_size, self.maxLength))\n",
    "        self.lengths   = tf.placeholder(tf.int32, (self.batch_size,))\n",
    "        self.labels    = tf.placeholder(tf.int32, (self.batch_size, self.maxLength))\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        # Embed encoder input\n",
    "        self.enc_input = tf.contrib.layers.embed_sequence(\n",
    "            ids        = self.input,\n",
    "            vocab_size = self.vocab_size,\n",
    "            embed_dim  = self.embedding_size)\n",
    "\n",
    "        # Decoder input (GO + label + EOS)\n",
    "        eos = tf.one_hot(\n",
    "            indices  = self.lengths,\n",
    "            depth    = self.maxLength,\n",
    "            on_value = self.tokens['EOS'])\n",
    "        \n",
    "        self.add_eos = self.labels + eos\n",
    "        go_tokens = tf.constant(self.tokens['GO'], shape=[self.batch_size, 1])\n",
    "        pre_embed_dec_input = tf.concat((go_tokens, self.add_eos), 1)\n",
    "        \n",
    "        # Embed decoder input\n",
    "        self.dec_embed = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size]))\n",
    "        self.dec_input = tf.nn.embedding_lookup(self.dec_embed, pre_embed_dec_input)\n",
    "\n",
    "    def one_layer_cell(self):\n",
    "        return rnn.DropoutWrapper(rnn.LSTMCell(self.layer_size), self.keep_prob)\n",
    "    \n",
    "    def cell(self):\n",
    "        return rnn.MultiRNNCell([self.one_layer_cell() for _ in range(self.layers)])\n",
    "    \n",
    "    # Build the compute graph. First encoder, then decoder, then train/test ops\n",
    "    def build_graph(self):\n",
    "        \n",
    "        # Build the encoder\n",
    "        _, enc_state = tf.nn.dynamic_rnn(\n",
    "            cell            = self.cell(),\n",
    "            inputs          = self.enc_input,\n",
    "            sequence_length = self.lengths,\n",
    "            dtype           = tf.float32)\n",
    "\n",
    "        # Replicate the top-most encoder state for starting state of all layers in the decoder\n",
    "        dec_start_state = tuple(enc_state[-1] for _ in range(self.layers))\n",
    "        \n",
    "        # Output layer converts from layer size to vocab size\n",
    "        output = Dense(self.vocab_size,\n",
    "            kernel_initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "        \n",
    "        # Training decoder: scheduled sampling et al.\n",
    "        with tf.variable_scope(\"decode\"):\n",
    "            \n",
    "            train_helper = seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                    inputs               = self.dec_input,\n",
    "                    sequence_length      = self.lengths,\n",
    "                    embedding            = self.dec_embed,\n",
    "                    sampling_probability = 0.1)\n",
    "\n",
    "            train_decoder = seq2seq.BasicDecoder(\n",
    "                    cell          = self.cell(),\n",
    "                    helper        = train_helper,\n",
    "                    initial_state = dec_start_state,\n",
    "                    output_layer  = output)\n",
    "            \n",
    "            train_output, _, train_lengths = seq2seq.dynamic_decode(\n",
    "                    decoder            = train_decoder,\n",
    "                    maximum_iterations = self.maxLength)\n",
    "            \n",
    "        # The beam search decoder requires starting states for each beam\n",
    "        tiled = bsd.tile_batch(dec_start_state, self.beam_width)\n",
    "        \n",
    "        # Share weights with training decoder\n",
    "        with tf.variable_scope(\"decode\", reuse=True):\n",
    "            \n",
    "            test_decoder = seq2seq.BeamSearchDecoder(\n",
    "                    cell          = self.cell(),\n",
    "                    embedding     = self.dec_embed,\n",
    "                    start_tokens  = tf.constant(self.tokens['GO'], shape=self.batch_size),\n",
    "                    end_token     = self.tokens['EOS'],\n",
    "                    initial_state = tiled,\n",
    "                    beam_width    = self.beam_width,\n",
    "                    output_layer  = output)\n",
    "            \n",
    "            test_output, _, test_lengths = seq2seq.dynamic_decode(\n",
    "                    decoder            = test_decoder,\n",
    "                    maximum_iterations = self.maxLength)\n",
    "        \n",
    "        # Create train op. Add one to train lengths, to include EOS\n",
    "        mask = tf.sequence_mask(train_lengths + 1, self.maxLength - 1, dtype=tf.float32)\n",
    "        self.cost = seq2seq.sequence_loss(train_output.rnn_output, self.add_eos[:, :-1], mask)\n",
    "        self.train_op = tf.train.AdamOptimizer(0.001).minimize(self.cost)\n",
    "\n",
    "        # Create test error rate op. Remove one from lengths to exclude EOS\n",
    "        predicts = self.to_sparse(test_output.predicted_ids[:,:,0], test_lengths[:, 0] - 1)\n",
    "        labels = self.to_sparse(self.labels, self.lengths)\n",
    "        self.error_rate = tf.reduce_mean(tf.edit_distance(predicts, labels))\n",
    "\n",
    "    # Convert a dense matrix into a sparse matrix (for e.g. edit_distance)\n",
    "    def to_sparse(self, tensor, lengths):\n",
    "        mask = tf.sequence_mask(lengths, self.maxLength)\n",
    "        indices = tf.to_int64(tf.where(tf.equal(mask, True)))\n",
    "        values = tf.to_int32(tf.boolean_mask(tensor, mask))\n",
    "        shape = tf.to_int64(tf.shape(tensor))\n",
    "        return tf.SparseTensor(indices, values, shape)\n",
    "\n",
    "    # Divide training samples into batches\n",
    "    def batchify(self):\n",
    "\n",
    "        for i in range(self.samples // self.batch_size):\n",
    "            yield self.next_batch(i)\n",
    "\n",
    "    # Create a single batch at i * batch_size\n",
    "    def next_batch(self, i):\n",
    "\n",
    "        start = i * self.batch_size\n",
    "        stop = (i+1) * self.batch_size\n",
    "\n",
    "        batch = {\n",
    "                self.input:     self.data[start:stop],\n",
    "                self.lengths:   self.dataLens[start:stop],\n",
    "                self.labels:    self.dataLabels[start:stop],\n",
    "                self.keep_prob: 1. - self.dropout\n",
    "        }\n",
    "\n",
    "        return batch\n",
    "\n",
    "    # Create a random test batch\n",
    "    def test_batch(self):\n",
    "\n",
    "        data = np.random.randint(\n",
    "            low  = len(self.tokens),\n",
    "            high = self.vocab_size,\n",
    "            size = (self.batch_size, self.maxLength))\n",
    "        \n",
    "        dataLens = np.random.randint(\n",
    "            low  = self.minLength,\n",
    "            high = self.maxLength,\n",
    "            size = self.batch_size)\n",
    "        \n",
    "        dataLabels = np.zeros_like(data)\n",
    "        for i in range(len(data)):\n",
    "            data[i, dataLens[i]:] = self.tokens['PAD']\n",
    "            dataLabels[i, :dataLens[i]] = np.sort(data[i, :dataLens[i]])\n",
    "\n",
    "        return {\n",
    "                self.input: data,\n",
    "                self.lengths: dataLens,\n",
    "                self.labels: dataLabels,\n",
    "                self.keep_prob: 1.\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a main method that uses this class! We'll train for 50 epochs and see how good our network gets at sorting integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 2.99814680338 test error: 0.876484\n",
      "Epoch 2 train loss: 2.45805616617 test error: 0.788333\n",
      "Epoch 3 train loss: 2.20980695486 test error: 0.743317\n",
      "Epoch 4 train loss: 2.05684555173 test error: 0.659405\n",
      "Epoch 5 train loss: 1.9409812361 test error: 0.619754\n",
      "Epoch 6 train loss: 1.85207181394 test error: 0.566563\n",
      "Epoch 7 train loss: 1.76751477897 test error: 0.57873\n",
      "Epoch 8 train loss: 1.68995686829 test error: 0.514579\n",
      "Epoch 9 train loss: 1.6139397037 test error: 0.527627\n",
      "Epoch 10 train loss: 1.55151514351 test error: 0.426175\n",
      "Epoch 11 train loss: 1.48051525295 test error: 0.446119\n",
      "Epoch 12 train loss: 1.40756380618 test error: 0.456683\n",
      "Epoch 13 train loss: 1.33944719374 test error: 0.394302\n",
      "Epoch 14 train loss: 1.26880092859 test error: 0.37119\n",
      "Epoch 15 train loss: 1.19582398295 test error: 0.307079\n",
      "Epoch 16 train loss: 1.12442905426 test error: 0.278095\n",
      "Epoch 17 train loss: 1.06169075131 test error: 0.253405\n",
      "Epoch 18 train loss: 0.991297113895 test error: 0.251937\n",
      "Epoch 19 train loss: 0.933868030012 test error: 0.237333\n",
      "Epoch 20 train loss: 0.874547387064 test error: 0.159865\n",
      "Epoch 21 train loss: 0.823280185759 test error: 0.150619\n",
      "Epoch 22 train loss: 0.765027674735 test error: 0.136246\n",
      "Epoch 23 train loss: 0.717253027558 test error: 0.10204\n",
      "Epoch 24 train loss: 0.670188134313 test error: 0.0975159\n",
      "Epoch 25 train loss: 0.627613121569 test error: 0.096373\n",
      "Epoch 26 train loss: 0.589763466567 test error: 0.0970635\n",
      "Epoch 27 train loss: 0.547945173979 test error: 0.0707381\n",
      "Epoch 28 train loss: 0.512827165127 test error: 0.0482857\n",
      "Epoch 29 train loss: 0.484260681868 test error: 0.0260635\n",
      "Epoch 30 train loss: 0.451367207021 test error: 0.022381\n",
      "Epoch 31 train loss: 0.412789892554 test error: 0.0499921\n",
      "Epoch 32 train loss: 0.393804501444 test error: 0.0226905\n",
      "Epoch 33 train loss: 0.35466650635 test error: 0.0176587\n",
      "Epoch 34 train loss: 0.344183525816 test error: 0.0180556\n",
      "Epoch 35 train loss: 0.318917696849 test error: 0.00980159\n",
      "Epoch 36 train loss: 0.298988836259 test error: 0.0102778\n",
      "Epoch 37 train loss: 0.283448895216 test error: 0.0025\n",
      "Epoch 38 train loss: 0.269724263996 test error: 0.00333333\n",
      "Epoch 39 train loss: 0.247967587039 test error: 0.00285714\n",
      "Epoch 40 train loss: 0.236626250222 test error: 0.0\n",
      "Epoch 41 train loss: 0.225658781752 test error: 0.0115\n",
      "Epoch 42 train loss: 0.213172226623 test error: 0.0025\n",
      "Epoch 43 train loss: 0.204786786959 test error: 0.0025\n",
      "Epoch 44 train loss: 0.193913319595 test error: 0.00222222\n",
      "Epoch 45 train loss: 0.185452427752 test error: 0.0\n",
      "Epoch 46 train loss: 0.172224486545 test error: 0.0\n",
      "Epoch 47 train loss: 0.168344544917 test error: 0.00472222\n",
      "Epoch 48 train loss: 0.161166814268 test error: 0.0025\n",
      "Epoch 49 train loss: 0.156258543693 test error: 0.00444444\n",
      "Epoch 50 train loss: 0.147649496719 test error: 0.0025\n"
     ]
    }
   ],
   "source": [
    "s2s = seq2seq_example()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(50):\n",
    "        \n",
    "        # Keep track of average train cost for this epoch\n",
    "        train_cost = 0\n",
    "        for batch in s2s.batchify():\n",
    "            train_cost += sess.run([s2s.train_op, s2s.cost], batch)[1]\n",
    "        train_cost /= s2s.samples / s2s.batch_size\n",
    "        \n",
    "        # Test time\n",
    "        er = sess.run(s2s.error_rate, s2s.test_batch())\n",
    "        \n",
    "        print(\"Epoch\", (epoch + 1), \"train loss:\", train_cost, \"test error:\", er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An error rate of 0 is pretty good, I'd say! That's all there is to it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
