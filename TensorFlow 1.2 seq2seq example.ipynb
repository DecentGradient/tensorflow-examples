{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 1.2 seq2seq example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there seems to be a dearth of up-to-date tensorflow examples on how to use the seq2seq module in contrib, I've decided to post this code online. It is based primarily on this tutorial: [Udacity's sequence to sequence implementation](https://github.com/udacity/deep-learning/blob/master/seq2seq/sequence_to_sequence_implementation.ipynb)\n",
    "\n",
    "This example takes a list of numbers and sorts it. There are multiple updates from the Udacity example, such as scheduled sampling, beam search, and error rate calculation. You will best understand what is going on in this example code if you already have a good background in TensorFlow and seq2seq networks.\n",
    "\n",
    "Unfortunately, Jupyter doesn't work well with classes, so I will have to put most of the code in a single cell. The comments should describe what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import numpy as np\n",
    "\n",
    "class seq2seq_example:\n",
    "\n",
    "    # Constants\n",
    "    tokens         = {\"PAD\": 0, \"EOS\": 1, \"GO\": 2, \"UNK\": 3}\n",
    "    minLength      = 5\n",
    "    maxLength      = 10\n",
    "    samples        = 10000\n",
    "    vocab_size     = 50\n",
    "    embedding_size = 15\n",
    "    dropout        = 0.3\n",
    "    layers         = 2\n",
    "    layer_size     = 100\n",
    "    batch_size     = 50\n",
    "    beam_width     = 4\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Random integers up to the vocab_size (not including reserved integers)\n",
    "        self.data = np.random.randint(\n",
    "            low  = len(self.tokens),\n",
    "            high = self.vocab_size,\n",
    "            size = (self.samples, self.maxLength))\n",
    "        \n",
    "        # Assign a random length to each sequence from minLength to maxLength\n",
    "        self.dataLens = np.random.randint(\n",
    "            low  = self.minLength,\n",
    "            high = self.maxLength,\n",
    "            size = self.samples)\n",
    "        \n",
    "        # Create labels by sorting the original data\n",
    "        self.dataLabels = np.ones_like(self.data) * self.tokens['PAD']\n",
    "        for i in range(len(self.data)):\n",
    "            self.data[i, self.dataLens[i]:] = self.tokens['PAD']\n",
    "            self.dataLabels[i, :self.dataLens[i]] = np.sort(self.data[i, :self.dataLens[i]])\n",
    "       \n",
    "        # Make placeholders and stuff\n",
    "        self.make_inputs()\n",
    "\n",
    "        # Build the compute graph\n",
    "        self.build_graph()\n",
    "\n",
    "    # Create the inputs to the graph (placeholders and stuff)\n",
    "    def make_inputs(self):\n",
    "        self.input     = tf.placeholder(tf.int32, (self.batch_size, self.maxLength))\n",
    "        self.lengths   = tf.placeholder(tf.int32, (self.batch_size,))\n",
    "        self.labels    = tf.placeholder(tf.int32, (self.batch_size, self.maxLength))\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        # Embed encoder input\n",
    "        self.enc_input = tf.contrib.layers.embed_sequence(\n",
    "            ids        = self.input,\n",
    "            vocab_size = self.vocab_size,\n",
    "            embed_dim  = self.embedding_size)\n",
    "\n",
    "        # Decoder input (GO + label + EOS)\n",
    "        eos = tf.one_hot(\n",
    "            indices  = self.lengths,\n",
    "            depth    = self.maxLength,\n",
    "            on_value = self.tokens['EOS'])\n",
    "        \n",
    "        self.add_eos = self.labels + eos\n",
    "        go_tokens = tf.constant(self.tokens['GO'], shape=[self.batch_size, 1])\n",
    "        pre_embed_dec_input = tf.concat((go_tokens, self.add_eos), 1)\n",
    "        \n",
    "        # Embed decoder input\n",
    "        self.dec_embed = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size]))\n",
    "        self.dec_input = tf.nn.embedding_lookup(self.dec_embed, pre_embed_dec_input)\n",
    "\n",
    "    def one_layer_cell(self):\n",
    "        return rnn.DropoutWrapper(rnn.LSTMCell(self.layer_size), self.keep_prob)\n",
    "    \n",
    "    def cell(self):\n",
    "        return rnn.MultiRNNCell([self.one_layer_cell() for _ in range(self.layers)])\n",
    "    \n",
    "    # Build the compute graph. First encoder, then decoder, then train/test ops\n",
    "    def build_graph(self):\n",
    "        \n",
    "        # Build the encoder\n",
    "        _, enc_state = tf.nn.dynamic_rnn(\n",
    "            cell            = self.cell(),\n",
    "            inputs          = self.enc_input,\n",
    "            sequence_length = self.lengths,\n",
    "            dtype           = tf.float32)\n",
    "\n",
    "        # Replicate the top-most encoder state for starting state of all layers in the decoder\n",
    "        dec_start_state = tuple(enc_state[-1] for _ in range(self.layers))\n",
    "        \n",
    "        # Output layer converts from layer size to vocab size\n",
    "        output = Dense(self.vocab_size,\n",
    "            kernel_initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "        \n",
    "        # Training decoder: scheduled sampling et al.\n",
    "        with tf.variable_scope(\"decode\"):\n",
    "            \n",
    "            train_helper = seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                    inputs               = self.dec_input,\n",
    "                    sequence_length      = self.lengths,\n",
    "                    embedding            = self.dec_embed,\n",
    "                    sampling_probability = 0.1)\n",
    "\n",
    "            train_decoder = seq2seq.BasicDecoder(\n",
    "                    cell          = self.cell(),\n",
    "                    helper        = train_helper,\n",
    "                    initial_state = dec_start_state,\n",
    "                    output_layer  = output)\n",
    "            \n",
    "            train_output, _, train_lengths = seq2seq.dynamic_decode(\n",
    "                    decoder            = train_decoder,\n",
    "                    maximum_iterations = self.maxLength)\n",
    "            \n",
    "        # The beam search decoder requires starting states for each beam\n",
    "        tiled = seq2seq.tile_batch(dec_start_state, self.beam_width)\n",
    "        \n",
    "        # Share weights with training decoder\n",
    "        with tf.variable_scope(\"decode\", reuse=True):\n",
    "            \n",
    "            test_decoder = seq2seq.BeamSearchDecoder(\n",
    "                    cell          = self.cell(),\n",
    "                    embedding     = self.dec_embed,\n",
    "                    start_tokens  = tf.ones_like(self.lengths) * self.tokens['GO'],\n",
    "                    end_token     = self.tokens['EOS'],\n",
    "                    initial_state = tiled,\n",
    "                    beam_width    = self.beam_width,\n",
    "                    output_layer  = output)\n",
    "            \n",
    "            test_output, _, test_lengths = seq2seq.dynamic_decode(\n",
    "                    decoder            = test_decoder,\n",
    "                    maximum_iterations = self.maxLength)\n",
    "        \n",
    "        # Create train op. Add one to train lengths, to include EOS\n",
    "        mask = tf.sequence_mask(train_lengths + 1, self.maxLength - 1, dtype=tf.float32)\n",
    "        self.cost = seq2seq.sequence_loss(train_output.rnn_output, self.add_eos[:, :-1], mask)\n",
    "        self.train_op = tf.train.AdamOptimizer(0.001).minimize(self.cost)\n",
    "\n",
    "        # Create test error rate op. Remove one from lengths to exclude EOS\n",
    "        predicts = self.to_sparse(test_output.predicted_ids[:,:,0], test_lengths[:, 0] - 1)\n",
    "        labels = self.to_sparse(self.labels, self.lengths)\n",
    "        self.error_rate = tf.reduce_mean(tf.edit_distance(predicts, labels))\n",
    "\n",
    "    # Convert a dense matrix into a sparse matrix (for e.g. edit_distance)\n",
    "    def to_sparse(self, tensor, lengths):\n",
    "        mask = tf.sequence_mask(lengths, self.maxLength)\n",
    "        indices = tf.to_int64(tf.where(tf.equal(mask, True)))\n",
    "        values = tf.to_int32(tf.boolean_mask(tensor, mask))\n",
    "        shape = tf.to_int64(tf.shape(tensor))\n",
    "        return tf.SparseTensor(indices, values, shape)\n",
    "\n",
    "    # Divide training samples into batches\n",
    "    def batchify(self):\n",
    "\n",
    "        for i in range(self.samples // self.batch_size):\n",
    "            yield self.next_batch(i)\n",
    "\n",
    "    # Create a single batch at i * batch_size\n",
    "    def next_batch(self, i):\n",
    "\n",
    "        start = i * self.batch_size\n",
    "        stop = (i+1) * self.batch_size\n",
    "\n",
    "        batch = {\n",
    "                self.input:     self.data[start:stop],\n",
    "                self.lengths:   self.dataLens[start:stop],\n",
    "                self.labels:    self.dataLabels[start:stop],\n",
    "                self.keep_prob: 1. - self.dropout\n",
    "        }\n",
    "\n",
    "        return batch\n",
    "\n",
    "    # Create a random test batch\n",
    "    def test_batch(self):\n",
    "\n",
    "        data = np.random.randint(\n",
    "            low  = len(self.tokens),\n",
    "            high = self.vocab_size,\n",
    "            size = (self.batch_size, self.maxLength))\n",
    "        \n",
    "        dataLens = np.random.randint(\n",
    "            low  = self.minLength,\n",
    "            high = self.maxLength,\n",
    "            size = self.batch_size)\n",
    "        \n",
    "        dataLabels = np.zeros_like(data)\n",
    "        for i in range(len(data)):\n",
    "            data[i, dataLens[i]:] = self.tokens['PAD']\n",
    "            dataLabels[i, :dataLens[i]] = np.sort(data[i, :dataLens[i]])\n",
    "\n",
    "        return {\n",
    "                self.input: data,\n",
    "                self.lengths: dataLens,\n",
    "                self.labels: dataLabels,\n",
    "                self.keep_prob: 1.\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a main method that uses this class! We'll train for 50 epochs and see how good our network gets at sorting integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "s2s = seq2seq_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 2.95076080084 test error: 0.85696\n",
      "Epoch 2 train loss: 2.36709939957 test error: 0.758524\n",
      "Epoch 3 train loss: 2.13378504694 test error: 0.641881\n",
      "Epoch 4 train loss: 1.9949222517 test error: 0.633294\n",
      "Epoch 5 train loss: 1.90027783036 test error: 0.56\n",
      "Epoch 6 train loss: 1.82952281773 test error: 0.548722\n",
      "Epoch 7 train loss: 1.75282929301 test error: 0.507857\n",
      "Epoch 8 train loss: 1.68457266033 test error: 0.554952\n",
      "Epoch 9 train loss: 1.61335900486 test error: 0.455817\n",
      "Epoch 10 train loss: 1.53946608305 test error: 0.463127\n",
      "Epoch 11 train loss: 1.45143870115 test error: 0.407873\n",
      "Epoch 12 train loss: 1.37976072252 test error: 0.361706\n",
      "Epoch 13 train loss: 1.29816036999 test error: 0.346222\n",
      "Epoch 14 train loss: 1.2223143208 test error: 0.29573\n",
      "Epoch 15 train loss: 1.14952869415 test error: 0.274294\n",
      "Epoch 16 train loss: 1.08034382433 test error: 0.260532\n",
      "Epoch 17 train loss: 1.01636653513 test error: 0.226746\n",
      "Epoch 18 train loss: 0.9518717134 test error: 0.216444\n",
      "Epoch 19 train loss: 0.885806947052 test error: 0.143278\n",
      "Epoch 20 train loss: 0.823250456452 test error: 0.151492\n",
      "Epoch 21 train loss: 0.767502086759 test error: 0.121365\n",
      "Epoch 22 train loss: 0.716034157276 test error: 0.0887222\n",
      "Epoch 23 train loss: 0.660793935657 test error: 0.0922064\n",
      "Epoch 24 train loss: 0.616170864701 test error: 0.0450397\n",
      "Epoch 25 train loss: 0.56673371762 test error: 0.0605\n",
      "Epoch 26 train loss: 0.523576325923 test error: 0.0494841\n",
      "Epoch 27 train loss: 0.488338284194 test error: 0.0320238\n",
      "Epoch 28 train loss: 0.446691936255 test error: 0.0209127\n",
      "Epoch 29 train loss: 0.420865124911 test error: 0.0259921\n",
      "Epoch 30 train loss: 0.39138165921 test error: 0.0155556\n",
      "Epoch 31 train loss: 0.373799180686 test error: 0.0166667\n",
      "Epoch 32 train loss: 0.340164480209 test error: 0.0165079\n",
      "Epoch 33 train loss: 0.324856189862 test error: 0.0227381\n",
      "Epoch 34 train loss: 0.301365136802 test error: 0.0302698\n",
      "Epoch 35 train loss: 0.283039446622 test error: 0.008\n",
      "Epoch 36 train loss: 0.265495986715 test error: 0.0177778\n",
      "Epoch 37 train loss: 0.248255459592 test error: 0.0\n",
      "Epoch 38 train loss: 0.240867633745 test error: 0.0\n",
      "Epoch 39 train loss: 0.227679889798 test error: 0.00333333\n",
      "Epoch 40 train loss: 0.210370197892 test error: 0.00222222\n",
      "Epoch 41 train loss: 0.207778116688 test error: 0.00694444\n",
      "Epoch 42 train loss: 0.18944136288 test error: 0.0\n",
      "Epoch 43 train loss: 0.190103459507 test error: 0.0\n",
      "Epoch 44 train loss: 0.175305638835 test error: 0.0\n",
      "Epoch 45 train loss: 0.16836359445 test error: 0.0123016\n",
      "Epoch 46 train loss: 0.165145705156 test error: 0.00285714\n",
      "Epoch 47 train loss: 0.154247292094 test error: 0.0104365\n",
      "Epoch 48 train loss: 0.155873598568 test error: 0.00694444\n",
      "Epoch 49 train loss: 0.147646608613 test error: 0.0025\n",
      "Epoch 50 train loss: 0.144966577999 test error: 0.00222222\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(50):\n",
    "        \n",
    "        # Keep track of average train cost for this epoch\n",
    "        train_cost = 0\n",
    "        for batch in s2s.batchify():\n",
    "            train_cost += sess.run([s2s.train_op, s2s.cost], batch)[1]\n",
    "        train_cost /= s2s.samples / s2s.batch_size\n",
    "        \n",
    "        # Test time\n",
    "        er = sess.run(s2s.error_rate, s2s.test_batch())\n",
    "        \n",
    "        print(\"Epoch\", (epoch + 1), \"train loss:\", train_cost, \"test error:\", er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An error rate of 0 is pretty good, I'd say! That's all there is to it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
