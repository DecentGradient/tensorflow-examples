{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 1.2 seq2seq example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there seems to be a dearth of up-to-date tensorflow examples on how to use the seq2seq module in contrib, I've decided to post this code online. It is based primarily on this tutorial: [Udacity's sequence to sequence implementation](https://github.com/udacity/deep-learning/blob/master/seq2seq/sequence_to_sequence_implementation.ipynb)\n",
    "\n",
    "This example takes a list of numbers and sorts it. There are multiple updates from the Udacity example, such as scheduled sampling, beam search, attention, and error rate calculation. You will best understand what is going on in this example code if you already have a good background in TensorFlow and seq2seq networks.\n",
    "\n",
    "Unfortunately, Jupyter doesn't work well with classes, so I will have to put most of the code in a single cell. The comments should describe what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import numpy as np\n",
    "\n",
    "class seq2seq_example:\n",
    "\n",
    "    # Constants\n",
    "    tokens         = {\"PAD\": 0, \"EOS\": 1, \"GO\": 2, \"UNK\": 3}\n",
    "    minLength      = 5\n",
    "    maxLength      = 10\n",
    "    samples        = 10000\n",
    "    vocab_size     = 50\n",
    "    embedding_size = 15\n",
    "    dropout        = 0.3\n",
    "    layers         = 2\n",
    "    layer_size     = 100\n",
    "    batch_size     = 50\n",
    "    beam_width     = 4\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Random integers up to the vocab_size (not including reserved integers)\n",
    "        self.data = np.random.randint(\n",
    "            low  = len(self.tokens),\n",
    "            high = self.vocab_size,\n",
    "            size = (self.samples, self.maxLength))\n",
    "        \n",
    "        # Assign a random length to each sequence from minLength to maxLength\n",
    "        self.dataLens = np.random.randint(\n",
    "            low  = self.minLength,\n",
    "            high = self.maxLength,\n",
    "            size = self.samples)\n",
    "        \n",
    "        # Create labels by sorting the original data\n",
    "        self.dataLabels = np.ones_like(self.data) * self.tokens['PAD']\n",
    "        for i in range(len(self.data)):\n",
    "            self.data[i, self.dataLens[i]:] = self.tokens['PAD']\n",
    "            self.dataLabels[i, :self.dataLens[i]] = np.sort(self.data[i, :self.dataLens[i]])\n",
    "       \n",
    "        # Make placeholders and stuff\n",
    "        self.make_inputs()\n",
    "\n",
    "        # Build the compute graph\n",
    "        self.build_graph()\n",
    "\n",
    "    # Create the inputs to the graph (placeholders and stuff)\n",
    "    def make_inputs(self):\n",
    "        self.input     = tf.placeholder(tf.int32, (self.batch_size, self.maxLength))\n",
    "        self.lengths   = tf.placeholder(tf.int32, (self.batch_size,))\n",
    "        self.labels    = tf.placeholder(tf.int32, (self.batch_size, self.maxLength))\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        # Embed encoder input\n",
    "        self.enc_input = tf.contrib.layers.embed_sequence(\n",
    "            ids        = self.input,\n",
    "            vocab_size = self.vocab_size,\n",
    "            embed_dim  = self.embedding_size)\n",
    "\n",
    "        # Decoder input (GO + label + EOS)\n",
    "        eos = tf.one_hot(\n",
    "            indices  = self.lengths,\n",
    "            depth    = self.maxLength,\n",
    "            on_value = self.tokens['EOS'])\n",
    "        \n",
    "        self.add_eos = self.labels + eos\n",
    "        go_tokens = tf.constant(self.tokens['GO'], shape=[self.batch_size, 1])\n",
    "        pre_embed_dec_input = tf.concat((go_tokens, self.add_eos), 1)\n",
    "        \n",
    "        # Embed decoder input\n",
    "        self.dec_embed = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size]))\n",
    "        self.dec_input = tf.nn.embedding_lookup(self.dec_embed, pre_embed_dec_input)\n",
    "\n",
    "    def one_layer_cell(self):\n",
    "        return rnn.DropoutWrapper(rnn.LSTMCell(self.layer_size), self.keep_prob)\n",
    "    \n",
    "    def cell(self):\n",
    "        return rnn.MultiRNNCell([self.one_layer_cell() for _ in range(self.layers)])\n",
    "    \n",
    "    def decoder_cell(self, inputs, lengths):\n",
    "        attention_mechanism = seq2seq.LuongAttention(\n",
    "                num_units              = self.layer_size,\n",
    "                memory                 = inputs,\n",
    "                memory_sequence_length = lengths,\n",
    "                scale                  = True)\n",
    "\n",
    "        return seq2seq.AttentionWrapper(\n",
    "                cell                 = self.cell(),\n",
    "                attention_mechanism  = attention_mechanism,\n",
    "                attention_layer_size = self.layer_size)\n",
    "    \n",
    "    # Build the compute graph. First encoder, then decoder, then train/test ops\n",
    "    def build_graph(self):\n",
    "        \n",
    "        # Build the encoder\n",
    "        enc_outputs, enc_state = tf.nn.dynamic_rnn(\n",
    "            cell            = self.cell(),\n",
    "            inputs          = self.enc_input,\n",
    "            sequence_length = self.lengths,\n",
    "            dtype           = tf.float32)\n",
    "\n",
    "        # Replicate the top-most encoder state for starting state of all layers in the decoder\n",
    "        dec_start_state = tuple(enc_state[-1] for _ in range(self.layers))\n",
    "        \n",
    "        # Output layer converts from layer size to vocab size\n",
    "        output = Dense(self.vocab_size,\n",
    "            kernel_initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "        \n",
    "        # Training decoder: scheduled sampling et al.\n",
    "        with tf.variable_scope(\"decode\"):\n",
    "            \n",
    "            cell = self.decoder_cell(enc_outputs, self.lengths)\n",
    "            init_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "            init_state = init_state.clone(cell_state=dec_start_state)\n",
    "            \n",
    "            train_helper = seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                    inputs               = self.dec_input,\n",
    "                    sequence_length      = self.lengths,\n",
    "                    embedding            = self.dec_embed,\n",
    "                    sampling_probability = 0.1)\n",
    "\n",
    "            train_decoder = seq2seq.BasicDecoder(\n",
    "                    cell          = cell,\n",
    "                    helper        = train_helper,\n",
    "                    initial_state = init_state,\n",
    "                    output_layer  = output)\n",
    "            \n",
    "            train_output, _, train_lengths = seq2seq.dynamic_decode(\n",
    "                    decoder            = train_decoder,\n",
    "                    maximum_iterations = self.maxLength)\n",
    "        \n",
    "        # Tile inputs for beam search decoder\n",
    "        dec_start_state = seq2seq.tile_batch(dec_start_state, self.beam_width)\n",
    "        enc_outputs = seq2seq.tile_batch(enc_outputs, self.beam_width)\n",
    "        lengths = seq2seq.tile_batch(self.lengths, self.beam_width)\n",
    "        \n",
    "        # Share weights with training decoder\n",
    "        with tf.variable_scope(\"decode\", reuse=True):\n",
    "            \n",
    "            cell = self.decoder_cell(enc_outputs, lengths)\n",
    "            init_state = cell.zero_state(self.batch_size * self.beam_width, tf.float32)\n",
    "            init_state = init_state.clone(cell_state=dec_start_state)\n",
    "            \n",
    "            test_decoder = seq2seq.BeamSearchDecoder(\n",
    "                    cell          = cell,\n",
    "                    embedding     = self.dec_embed,\n",
    "                    start_tokens  = tf.ones_like(self.lengths) * self.tokens['GO'],\n",
    "                    end_token     = self.tokens['EOS'],\n",
    "                    initial_state = init_state,\n",
    "                    beam_width    = self.beam_width,\n",
    "                    output_layer  = output)\n",
    "            \n",
    "            test_output, _, test_lengths = seq2seq.dynamic_decode(\n",
    "                    decoder            = test_decoder,\n",
    "                    maximum_iterations = self.maxLength)\n",
    "        \n",
    "        # Create train op. Add one to train lengths, to include EOS\n",
    "        mask = tf.sequence_mask(train_lengths + 1, self.maxLength - 1, dtype=tf.float32)\n",
    "        self.cost = seq2seq.sequence_loss(train_output.rnn_output, self.add_eos[:, :-1], mask)\n",
    "        self.train_op = tf.train.AdamOptimizer(0.001).minimize(self.cost)\n",
    "\n",
    "        # Create test error rate op. Remove one from lengths to exclude EOS\n",
    "        predicts = self.to_sparse(test_output.predicted_ids[:,:,0], test_lengths[:, 0] - 1)\n",
    "        labels = self.to_sparse(self.labels, self.lengths)\n",
    "        self.error_rate = tf.reduce_mean(tf.edit_distance(predicts, labels))\n",
    "\n",
    "    # Convert a dense matrix into a sparse matrix (for e.g. edit_distance)\n",
    "    def to_sparse(self, tensor, lengths):\n",
    "        mask = tf.sequence_mask(lengths, self.maxLength)\n",
    "        indices = tf.to_int64(tf.where(tf.equal(mask, True)))\n",
    "        values = tf.to_int32(tf.boolean_mask(tensor, mask))\n",
    "        shape = tf.to_int64(tf.shape(tensor))\n",
    "        return tf.SparseTensor(indices, values, shape)\n",
    "\n",
    "    # Divide training samples into batches\n",
    "    def batchify(self):\n",
    "\n",
    "        for i in range(self.samples // self.batch_size):\n",
    "            yield self.next_batch(i)\n",
    "\n",
    "    # Create a single batch at i * batch_size\n",
    "    def next_batch(self, i):\n",
    "\n",
    "        start = i * self.batch_size\n",
    "        stop = (i+1) * self.batch_size\n",
    "\n",
    "        batch = {\n",
    "                self.input:     self.data[start:stop],\n",
    "                self.lengths:   self.dataLens[start:stop],\n",
    "                self.labels:    self.dataLabels[start:stop],\n",
    "                self.keep_prob: 1. - self.dropout\n",
    "        }\n",
    "\n",
    "        return batch\n",
    "\n",
    "    # Create a random test batch\n",
    "    def test_batch(self):\n",
    "\n",
    "        data = np.random.randint(\n",
    "            low  = len(self.tokens),\n",
    "            high = self.vocab_size,\n",
    "            size = (self.batch_size, self.maxLength))\n",
    "        \n",
    "        dataLens = np.random.randint(\n",
    "            low  = self.minLength,\n",
    "            high = self.maxLength,\n",
    "            size = self.batch_size)\n",
    "        \n",
    "        dataLabels = np.zeros_like(data)\n",
    "        for i in range(len(data)):\n",
    "            data[i, dataLens[i]:] = self.tokens['PAD']\n",
    "            dataLabels[i, :dataLens[i]] = np.sort(data[i, :dataLens[i]])\n",
    "\n",
    "        return {\n",
    "                self.input: data,\n",
    "                self.lengths: dataLens,\n",
    "                self.labels: dataLabels,\n",
    "                self.keep_prob: 1.\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a main method that uses this class! We'll train for 50 epochs and see how good our network gets at sorting integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "s2s = seq2seq_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 2.91986195683 test error: 0.834714\n",
      "Epoch 2 train loss: 2.27854833126 test error: 0.742071\n",
      "Epoch 3 train loss: 2.05610381961 test error: 0.609936\n",
      "Epoch 4 train loss: 1.92091192007 test error: 0.544151\n",
      "Epoch 5 train loss: 1.79896817088 test error: 0.484119\n",
      "Epoch 6 train loss: 1.65584457636 test error: 0.349063\n",
      "Epoch 7 train loss: 1.4184307152 test error: 0.197675\n",
      "Epoch 8 train loss: 1.11107378602 test error: 0.0938571\n",
      "Epoch 9 train loss: 0.871339265406 test error: 0.0533254\n",
      "Epoch 10 train loss: 0.713713488728 test error: 0.0281667\n",
      "Epoch 11 train loss: 0.604298673123 test error: 0.0258333\n",
      "Epoch 12 train loss: 0.544282832742 test error: 0.0402698\n",
      "Epoch 13 train loss: 0.486932658702 test error: 0.0130159\n",
      "Epoch 14 train loss: 0.449633491188 test error: 0.00777778\n",
      "Epoch 15 train loss: 0.409089321047 test error: 0.0303889\n",
      "Epoch 16 train loss: 0.394229536355 test error: 0.0158333\n",
      "Epoch 17 train loss: 0.370213930979 test error: 0.0108333\n",
      "Epoch 18 train loss: 0.342018755376 test error: 0.00952381\n",
      "Epoch 19 train loss: 0.325859643742 test error: 0.0\n",
      "Epoch 20 train loss: 0.308327895328 test error: 0.00444444\n",
      "Epoch 21 train loss: 0.290842123702 test error: 0.0084127\n",
      "Epoch 22 train loss: 0.284117041528 test error: 0.00869048\n",
      "Epoch 23 train loss: 0.275989980996 test error: 0.00777778\n",
      "Epoch 24 train loss: 0.269462534711 test error: 0.00666667\n",
      "Epoch 25 train loss: 0.252057261914 test error: 0.00507937\n",
      "Epoch 26 train loss: 0.247674267814 test error: 0.00285714\n",
      "Epoch 27 train loss: 0.232467229217 test error: 0.00805556\n",
      "Epoch 28 train loss: 0.228952821717 test error: 0.00888889\n",
      "Epoch 29 train loss: 0.224217796773 test error: 0.00222222\n",
      "Epoch 30 train loss: 0.210341431685 test error: 0.0\n",
      "Epoch 31 train loss: 0.201171869896 test error: 0.0025\n",
      "Epoch 32 train loss: 0.195612193421 test error: 0.00844445\n",
      "Epoch 33 train loss: 0.18933903683 test error: 0.00333333\n",
      "Epoch 34 train loss: 0.184589334577 test error: 0.0\n",
      "Epoch 35 train loss: 0.176255308613 test error: 0.00333333\n",
      "Epoch 36 train loss: 0.180171127692 test error: 0.0\n",
      "Epoch 37 train loss: 0.163527621329 test error: 0.0\n",
      "Epoch 38 train loss: 0.167020770088 test error: 0.0\n",
      "Epoch 39 train loss: 0.160414721444 test error: 0.00444444\n",
      "Epoch 40 train loss: 0.153287142739 test error: 0.0025\n",
      "Epoch 41 train loss: 0.15106973609 test error: 0.0\n",
      "Epoch 42 train loss: 0.149061797969 test error: 0.0\n",
      "Epoch 43 train loss: 0.149537268914 test error: 0.0\n",
      "Epoch 44 train loss: 0.139682257585 test error: 0.00730159\n",
      "Epoch 45 train loss: 0.137363640536 test error: 0.00222222\n",
      "Epoch 46 train loss: 0.135108639039 test error: 0.00472222\n",
      "Epoch 47 train loss: 0.128768154997 test error: 0.0025\n",
      "Epoch 48 train loss: 0.122774963211 test error: 0.0\n",
      "Epoch 49 train loss: 0.125188367758 test error: 0.0\n",
      "Epoch 50 train loss: 0.116121740155 test error: 0.00285714\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(50):\n",
    "        \n",
    "        # Keep track of average train cost for this epoch\n",
    "        train_cost = 0\n",
    "        for batch in s2s.batchify():\n",
    "            train_cost += sess.run([s2s.train_op, s2s.cost], batch)[1]\n",
    "        train_cost /= s2s.samples / s2s.batch_size\n",
    "        \n",
    "        # Test time\n",
    "        er = sess.run(s2s.error_rate, s2s.test_batch())\n",
    "        \n",
    "        print(\"Epoch\", (epoch + 1), \"train loss:\", train_cost, \"test error:\", er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An error rate of 0 is pretty good, I'd say! That's all there is to it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
